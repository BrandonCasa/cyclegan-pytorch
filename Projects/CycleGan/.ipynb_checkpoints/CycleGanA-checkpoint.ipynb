{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f57033ec-4c3e-4a45-a942-d281dd177097",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set up the input pipeline\n",
    "Install the tensorflow_examples package that enables importing of the generator and the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f707b89-9d1d-4e80-98cc-f0fd02d7d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "import time\n",
    "#  TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf76c10-7e47-4af6-9d44-0846fcc2e2bb",
   "metadata": {},
   "source": [
    "## Input Pipeline\n",
    "This tutorial trains a model to translate from images of horses, to images of zebras. You can find this dataset and similar ones here.\n",
    "\n",
    "As mentioned in the paper, apply random jittering and mirroring to the training dataset. These are some of the image augmentation techniques that avoids overfitting.\n",
    "\n",
    "This is similar to what was done in pix2pix\n",
    "* In random jittering, the image is resized to 286 x 286 and then randomly cropped to 256 x 256.\n",
    "* In random mirroring, the image is randomly flipped horizontally i.e. left to right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053ad096-3b94-4815-a159-880beb5234ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, metadata = tfds.load('cycle_gan/horse2zebra',\n",
    "                              with_info=True, as_supervised=True)\n",
    "\n",
    "train_horses, train_zebras = dataset['trainA'], dataset['trainB']\n",
    "test_horses, test_zebras = dataset['testA'], dataset['testB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d58e6e8-14e9-4dc0-8276-68d183984e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 1\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d029bd54-e2ce-454e-b1b5-398791df090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(image):\n",
    "  cropped_image = tf.image.random_crop(\n",
    "      image, size=[IMG_HEIGHT, IMG_WIDTH, 3])\n",
    "\n",
    "  return cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e043b1ec-d000-4299-9932-98e294295fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the images to [-1, 1]\n",
    "def normalize(image):\n",
    "  image = tf.cast(image, tf.float32)\n",
    "  image = (image / 127.5) - 1\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38819a13-e260-4ade-b2ba-1827e40ba7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_jitter(image):\n",
    "  # resizing to 286 x 286 x 3\n",
    "  image = tf.image.resize(image, [286, 286],\n",
    "                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "  # randomly cropping to 256 x 256 x 3\n",
    "  image = random_crop(image)\n",
    "\n",
    "  # random mirroring\n",
    "  image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713ecebe-1d17-4929-afb5-9356a06b5582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image_train(image, label):\n",
    "  image = random_jitter(image)\n",
    "  image = normalize(image)\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9ed425-d187-41e9-b049-bfc9e2ac09ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image_test(image, label):\n",
    "  image = normalize(image)\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905debad-ab73-46c9-af61-fd4f23351d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_horses = train_horses.cache().map(\n",
    "    preprocess_image_train, num_parallel_calls=AUTOTUNE).shuffle(\n",
    "    BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "train_zebras = train_zebras.cache().map(\n",
    "    preprocess_image_train, num_parallel_calls=AUTOTUNE).shuffle(\n",
    "    BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "test_horses = test_horses.map(\n",
    "    preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(\n",
    "    BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "test_zebras = test_zebras.map(\n",
    "    preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(\n",
    "    BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f02401a-c024-42d2-87db-698f3c4a475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_horse = next(iter(train_horses))\n",
    "sample_zebra = next(iter(train_zebras))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936e4aca-2015-4f8d-b937-5c6a1fca3093",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plt.title('Horse')\n",
    "plt.imshow(sample_horse[0] * 0.5 + 0.5)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Horse with random jitter')\n",
    "plt.imshow(random_jitter(sample_horse[0]) * 0.5 + 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d92fb1f-97a0-4d0a-b745-f12341a47b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plt.title('Zebra')\n",
    "plt.imshow(sample_zebra[0] * 0.5 + 0.5)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Zebra with random jitter')\n",
    "plt.imshow(random_jitter(sample_zebra[0]) * 0.5 + 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19852a7-02c6-4a98-90bb-27065130aa4d",
   "metadata": {},
   "source": [
    "## Import and reuse the Pix2Pix models\n",
    "Import the generator and the discriminator used in Pix2Pix via the installed tensorflow_examples package.\n",
    "\n",
    "The model architecture used in this tutorial is very similar to what was used in pix2pix. Some of the differences are:\n",
    "* Cyclegan uses instance normalization instead of batch normalization.\n",
    "* The CycleGAN paper uses a modified resnet based generator. This tutorial is using a modified unet generator for simplicity.\n",
    "\n",
    "There are 2 generators (G and F) and 2 discriminators (X and Y) being trained here.\n",
    "* Generator G learns to transform image X to image Y. \n",
    "* Generator F learns to transform image Y to image X. \n",
    "* Discriminator D_X learns to differentiate between image X and generated image X (F(Y)).\n",
    "* Discriminator D_Y learns to differentiate between image Y and generated image Y (G(X))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41db5dd-36d1-425b-a3b7-abade772a222",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 3\n",
    "\n",
    "generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
    "generator_f = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
    "\n",
    "discriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\n",
    "discriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddabba6-c03c-4359-9a49-e179995e6e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_zebra = generator_g(sample_horse)\n",
    "to_horse = generator_f(sample_zebra)\n",
    "plt.figure(figsize=(8, 8))\n",
    "contrast = 8\n",
    "\n",
    "imgs = [sample_horse, to_zebra, sample_zebra, to_horse]\n",
    "title = ['Horse', 'To Zebra', 'Zebra', 'To Horse']\n",
    "\n",
    "for i in range(len(imgs)):\n",
    "  plt.subplot(2, 2, i+1)\n",
    "  plt.title(title[i])\n",
    "  if i % 2 == 0:\n",
    "    plt.imshow(imgs[i][0] * 0.5 + 0.5)\n",
    "  else:\n",
    "    plt.imshow(imgs[i][0] * 0.5 * contrast + 0.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
